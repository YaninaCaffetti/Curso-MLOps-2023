{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ciencia de Datos e Inteligencia Artificial para la industria del software**\n",
    "\n",
    "# *Curso MLOps*\n",
    "\n",
    "## **Edición 2023**\n",
    "\n",
    "# 5. Web depoyment\n",
    "\n",
    "*Despliegue de modelo en la web*\n",
    "\n",
    "Por decirlo de alguna forma, si no puedes sacar tus modelos de aprendizaje automático de un *notebook*, probablemente no se utilizarán. Este artículo debería ayudarte a crear un **despliegue de tu modelo lo más rápido posible, para que puedas implementar tus modelos rapidamente**. Se trata de una habilidad importante, ya que significa que **no dependerás indispensablemente de un ingeniero/desarrollador de software para poder poner un modelo en producción.** \n",
    "\n",
    "Para ello utilizaremos ***Hugging Face***. \n",
    "\n",
    "### ¿Qué es Hugging Face 🤗?\n",
    "\n",
    "- Una [comunidad colaborativa](https://huggingface.co/) especialmente enfocada en modelos de lenguaje y otros recursos de Inteligencia Artificial (IA).\n",
    "- Ofrece repositorios para disponibilizar [modelos](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) y [demos](https://huggingface.co/spaces).\n",
    "- Además, ofrece varias librerías orientadas a la IA, particularmente al Aprendizaje Profundo (*Deep Learning*), entre las que destacan:\n",
    "    - [`transformers`](https://huggingface.co/docs/transformers): La que veremos en esta charla, para todo lo relacionado a Procesamiento del Lenguaje Natural (PLN) con grandes modelos de lenguaje (*Large Language Models*, LLMs).\n",
    "    - [`datasets`](https://huggingface.co/docs/datasets): Una librería con funcionalidades para el tratamiento de los conjuntos de datos a utilizar para entrenar o ajustar los LLMs.\n",
    "    - [`tokenizers`](https://huggingface.co/docs/tokenizers): Una librería para el proceso de \"tokenización\", i.e. la división de texto de manera discreta en palabras o subpalabras.\n",
    "- Hugging Face no sólo ofrece soluciones para PLN, sino también para imágenes, con librerías como [`diffusers`](https://huggingface.co/docs/diffusers), para la generación de imágenes:\n",
    "    - Lectura recomendada: [The Illustrated Stable Diffusion](http://jalammar.github.io/illustrated-stable-diffusion/)\n",
    "\n",
    "Huggin Face es una empresa que busca desarrollar herramientas de código abierto para el diseño, entrenamiento y puesta en producción de modelos de inteligencia artificial. Creada en 2016, empezó a ganar popularidad gracias a su librería transformers, la cual permite crear y entrenar redes neuronales con la arquitectura *Transformer* de manera sencilla e independiente del framework agnóstica, pudiendo usar en el backend tanto Pytorch como Tensorflo o JAX. Hoy en día, sin embargo, ofrece muchas otras funcionalidades tales como *Datasets*, Modelos pre-entrenados, herramientas para el entrenamiento y puesta en producción de modelos a escala, etc. Recientemente incluso han incluído en su catálogo la librería timm, para la generación de imágenes al estilo *DALL-E* o *Stable Diffusion*, indicando que Hugging Face está creciendo como comunidad, yendo más allá de los *Transfromers* y sus aplicaciones en tareas de lenguaje hacia otros campos como el de la visión artificial o el aprendizaje por refuerzo. \n",
    "\n",
    "## ¿Cómo empezar con Hugging Face 🤗?\n",
    "\n",
    "- Primero se [crea una cuenta en la página](https://huggingface.co/join).\n",
    "- Luego podemos [crear modelos](https://huggingface.co/new) a través del menú que se despliega de nuestro avatar.\n",
    "\n",
    "Este es el esquema que seguiremos:\n",
    "\n",
    "**1.** Crear una cuenta de Hugging Face 🤗\n",
    "* [crea una cuenta en la página](https://huggingface.co/join).\n",
    "\n",
    "**2.** Crear un *Space* en Hugging Face y configurarlo\n",
    "* Los Spaces son repositorios Git que alojan código de aplicación para demos de Machine Learning.\n",
    "\n",
    "a- Seleccione el SDK de Space: **Gradio**.\n",
    "b- Space hardware: **Free**.\n",
    "c- Public.\n",
    "d- Create space.\n",
    "\n",
    "\n",
    "**Archivos necesarios:**  \n",
    "\n",
    "* .gitattributes (Creado automaticamente)\n",
    "* README.md (Creado automaticamente)\n",
    "* app.py (app propiamente dicha que realizara la predicción)\n",
    "* model.pkl (modelo previamente entrenado)\n",
    "* requirements.txt (requirements del environment con el cual entrenamos nuestro modelo)\n",
    "\n",
    "\n",
    "**3.** Cargamos el modelo y el preprocesador previamente entrenados:\n",
    "\n",
    "* lin_reg.bin\n",
    "* preprocessor.b\n",
    "\n",
    "**4.** Crear y subir el archivo app.py que realizara el preprocesamiento y las predicciones:\n",
    "\n",
    "### **app.py** \n",
    "\n",
    "```bash\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import pathlib\n",
    "#plt = platform.system()\n",
    "#if plt == 'Linux': pathlib.WindowsPath = pathlib.PosixPath\n",
    "\n",
    "with open('lin_reg.bin', 'rb') as f_in:\n",
    "    (dv, model) = pickle.load(f_in)\n",
    "\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "    \n",
    "def predict(features):\n",
    "    X = dv.transform(features)\n",
    "    preds = model.predict(X)\n",
    "    return float(preds[0])\n",
    "\n",
    "def main(PULocationID,DOLocationID,trip_distance):\n",
    "    \"\"\"request input, preprocess it and make prediction\"\"\"\n",
    "    input_data = {\n",
    "    \"PULocationID\": PULocationID,\n",
    "    \"DOLocationID\": DOLocationID,\n",
    "    \"trip_distance\": trip_distance\n",
    "    }\n",
    "    features = prepare_features(input_data)\n",
    "    pred = predict(features)\n",
    "\n",
    "    result = {\n",
    "        'duration': pred\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "#create input and output objects\n",
    "#input\n",
    "input1 = gr.inputs.Number()\n",
    "input2 = gr.inputs.Number()\n",
    "input3 = gr.inputs.Number()\n",
    "\n",
    "#output object\n",
    "output = gr.outputs.Textbox() \n",
    "\n",
    "intf = gr.Interface(title = \"New York taxi duration prediction\",\n",
    "                    description = \"The objective of this project is to predict the duration of a taxi trip in the city of New York.\",\n",
    "                    fn=main, \n",
    "                    inputs=[input1,input2,input3], \n",
    "                    outputs=[output], \n",
    "                    live=True,\n",
    "                    enable_queue=True\n",
    "                    )\n",
    "intf.launch()\n",
    "```\n",
    "\n",
    "**5.** Crear y subir el archivo requirements.txt:\n",
    "\n",
    "### requirements.txt\n",
    "\n",
    "```bash\n",
    "pandas==2.1.1\n",
    "scikit-learn==1.3.1\n",
    "```\n",
    "\n",
    "**6.** Entrar a la ventana de App, esperar que se construya y realizar predicciones!!!.\n",
    "\n",
    "[Mi app](https://huggingface.co/spaces/martinnnuez/New_york_taxi_duration_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicación cada chunk del cógido app.py:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import pathlib\n",
    "```\n",
    "* Importación de bibliotecas: En esta sección, se importan las bibliotecas necesarias. Estas bibliotecas incluyen \"pickle\" para cargar el modelo previamente entrenado, \"pandas\" y \"numpy\" para el procesamiento de datos, y \"gradio\" para crear una interfaz de usuario para la aplicación.\n",
    "\n",
    "```python\n",
    "with open('lin_reg.bin', 'rb') as f_in:\n",
    "    (dv, model) = pickle.load(f_in)\n",
    "```\n",
    "* Carga del modelo: En esta sección, se abre el archivo \"lin_reg.bin\", que contiene un modelo previamente entrenado, se carga en las variables \"dv\" (vectorizador de características) y \"model\" (modelo de regresión lineal).\n",
    "\n",
    "```python\n",
    "def prepare_features(ride):\n",
    "    features = {}\n",
    "    features['PU_DO'] = '%s_%s' % (ride['PULocationID'], ride['DOLocationID'])\n",
    "    features['trip_distance'] = ride['trip_distance']\n",
    "    return features\n",
    "```\n",
    "* Función \"prepare_features\": Esta función toma un objeto \"ride\" como entrada y prepara las características necesarias para hacer una predicción. Crea un diccionario \"features\" que contiene las ubicaciones de recogida y entrega concatenadas en \"PU_DO\" y la distancia del viaje en \"trip_distance\". Luego, devuelve este diccionario de características.\n",
    "\n",
    "```python\n",
    "def predict(features):\n",
    "    X = dv.transform(features)\n",
    "    preds = model.predict(X)\n",
    "    return float(preds[0])\n",
    "```\n",
    "* Función \"predict\": Esta función toma un diccionario de características \"features\" como entrada. Utiliza el vectorizador de características \"dv\" para transformar las características en un formato adecuado y, a continuación, hace una predicción utilizando el modelo previamente cargado. Devuelve la duración del viaje predicha como un número flotante.\n",
    "\n",
    "```python\n",
    "def main(PULocationID, DOLocationID, trip_distance):\n",
    "    \"\"\"request input, preprocess it and make a prediction\"\"\"\n",
    "    input_data = {\n",
    "        \"PULocationID\": PULocationID,\n",
    "        \"DOLocationID\": DOLocationID,\n",
    "        \"trip_distance\": trip_distance\n",
    "    }\n",
    "    features = prepare_features(input_data)\n",
    "    pred = predict(features)\n",
    "\n",
    "    result = {\n",
    "        'duration': pred\n",
    "    }\n",
    "\n",
    "    return result\n",
    "```\n",
    "* Función \"main\": Esta función toma como entrada las ubicaciones de recogida (\"PULocationID\"), las ubicaciones de entrega (\"DOLocationID\") y la distancia del viaje (\"trip_distance\"). Luego, crea un diccionario \"input_data\" con estas entradas y llama a las funciones \"prepare_features\" y \"predict\" para obtener una predicción de la duración del viaje. La función devuelve el resultado en un diccionario con la clave \"duration\".\n",
    "\n",
    "```python\n",
    "# Creación de objetos de entrada y salida\n",
    "# Entradas\n",
    "input1 = gr.inputs.Number(label=\"PULocationID\")\n",
    "input2 = gr.inputs.Number(label=\"DOLocationID\")\n",
    "input3 = gr.inputs.Number(label=\"trip_distance\")\n",
    "\n",
    "# Salida\n",
    "output = gr.outputs.Textbox(label=\"Duración estimada del viaje\")\n",
    "\n",
    "intf = gr.Interface(\n",
    "    title=\"Predicción de duración de viaje en taxis de Nueva York\",\n",
    "    description=\"El objetivo de este proyecto es predecir la duración de un viaje en taxi en la ciudad de Nueva York.\",\n",
    "    fn=main,\n",
    "    inputs=[input1, input2, input3],\n",
    "    outputs=[output],\n",
    "    live=True,\n",
    "    enable_queue=True\n",
    ")\n",
    "intf.launch()\n",
    "```\n",
    "\n",
    "* Interfaz Gradio: En esta sección se crea la interfaz de usuario utilizando la biblioteca Gradio. Se definen objetos de entrada y salida para la interfaz, que incluyen etiquetas y tipos de datos. Luego, se crea la interfaz en sí, especificando el título, descripción, función \"main\" que realizará la predicción y los objetos de entrada y salida. La interfaz se inicia y se pone en marcha."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
